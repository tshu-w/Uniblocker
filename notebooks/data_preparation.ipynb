{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_id(row, df):\n",
    "    id1 = row[\"id1\"]\n",
    "    id2 = row[\"id2\"]\n",
    "    lst1 = df.index[df[\"id\"] == id1].tolist()\n",
    "    lst2 = df.index[df[\"id\"] == id2].tolist()\n",
    "    assert len(lst1) == len(lst2) == 1\n",
    "    idx1, idx2 = lst1[0], lst2[0]\n",
    "    if idx1 > idx2:\n",
    "        return id2, id1\n",
    "    else:\n",
    "        return id1, id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblp-scholoar\n",
    "folder = Path(\"../data/blocking/dblp-scholar/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"DBLP1.csv\", encoding=\"latin-1\")\n",
    "df1.to_csv(folder / \"1_dblp.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"Scholar.csv\")\n",
    "df2.to_csv(folder / \"2_scholar.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "df = pd.read_csv(folder / \"raw\" / \"DBLP-Scholar_perfectMapping.csv\")\n",
    "df.rename(columns={\"idDBLP\": \"id1\", \"idScholar\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblp-acm\n",
    "folder = Path(\"../data/blocking/dblp-acm/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"DBLP2.csv\", encoding=\"latin-1\")\n",
    "df1.to_csv(folder / \"1_dblp.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"ACM.csv\")\n",
    "df2.to_csv(folder / \"2_acm.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "df = pd.read_csv(folder / \"raw\" / \"DBLP-ACM_perfectMapping.csv\")\n",
    "df.rename(columns={\"idDBLP\": \"id1\", \"idACM\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon-google\n",
    "folder = Path(\"../data/blocking/amazon-google/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"Amazon.csv\", encoding=\"latin-1\")\n",
    "df1.drop(columns=[\"description\"], inplace=True)\n",
    "df1.to_csv(folder / \"1_amazon.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"GoogleProducts.csv\", encoding=\"latin-1\")\n",
    "df2.drop(columns=[\"description\"], inplace=True)\n",
    "df2.rename(columns={\"manufacturer\": \"factory\"}, inplace=True)\n",
    "df2.to_csv(folder / \"2_google.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "df = pd.read_csv(folder / \"raw\" / \"Amzon_GoogleProducts_perfectMapping.csv\")\n",
    "df.rename(columns={\"idAmazon\": \"id1\", \"idGoogleBase\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abt-buy_homo\n",
    "folder = Path(\"../data/blocking/abt-buy_homo/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"Abt.csv\", encoding=\"latin-1\")\n",
    "df1.drop(columns=[\"description\"], inplace=True)\n",
    "df1.to_csv(folder / \"1_abt.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"Buy.csv\")\n",
    "df2.drop(columns=[\"description\", \"manufacturer\"], inplace=True)  # drop manufacturer\n",
    "df2.to_csv(folder / \"2_buy.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df = pd.read_csv(folder / \"raw\" / \"abt_buy_perfectMapping.csv\")\n",
    "df.rename(columns={\"idAbt\": \"id1\", \"idBuy\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abt-buy_heter\n",
    "folder = Path(\"../data/blocking/abt-buy_heter/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"Abt.csv\", encoding=\"latin-1\")\n",
    "df1.drop(columns=[\"description\"], inplace=True)\n",
    "df1.rename(columns={\"name\": \"title\"}, inplace=True)\n",
    "df1.to_csv(folder / \"1_abt.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"Buy.csv\")\n",
    "df2.drop(columns=[\"description\"], inplace=True)\n",
    "df2.to_csv(folder / \"2_buy.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "df = pd.read_csv(folder / \"raw\" / \"abt_buy_perfectMapping.csv\")\n",
    "df.rename(columns={\"idAbt\": \"id1\", \"idBuy\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cora\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "folder = Path(\"../data/blocking/cora/\")\n",
    "tree = ElementTree.parse(folder / \"raw\" / \"cora.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "entities = []\n",
    "for elem in root:\n",
    "    entities.append({attr.tag: attr.text for attr in elem})\n",
    "\n",
    "df1 = pd.DataFrame(entities)\n",
    "idx = df1.index + 1\n",
    "df1.insert(0, \"id\", idx)\n",
    "df1.to_csv(folder / \"1_cora.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"cora_gold.csv\", sep=\";\")\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "# df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type='broadcast')\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fodors-zagats_homo\n",
    "folder = Path(\"../data/blocking/fodors-zagats_homo/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"fodors_zagat_raw_data\" / \"tableA.csv\")\n",
    "df1 = df1.map(lambda x: x.strip(\"'\").replace(\"\\\\'\", \"'\") if isinstance(x, str) else x)\n",
    "df1.to_csv(folder / \"1_fodors.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"fodors_zagat_raw_data\" / \"tableB.csv\")\n",
    "df2 = df2.map(lambda x: x.strip(\"'\").replace(\"\\\\'\", \"'\") if isinstance(x, str) else x)\n",
    "df2.to_csv(folder / \"2_zagats.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "df = pd.read_csv(folder / \"raw\" / \"fodors_zagat_raw_data\" / \"matches.csv\")\n",
    "df.rename(columns={\"fodors_id\": \"id1\", \"zagats_id\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fodors-zagats_heter\n",
    "folder = Path(\"../data/blocking/fodors-zagats_heter/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"fodors_zagat_raw_data\" / \"tableA.csv\")\n",
    "df1 = df1.map(lambda x: x.strip(\"'\").replace(\"\\\\'\", \"'\") if isinstance(x, str) else x)\n",
    "df1 = df1.rename(columns={\"name\": \"title\"})\n",
    "address = df1[\"addr\"] + \" \" + df1[\"city\"]\n",
    "df1 = df1.drop(columns=[\"addr\", \"city\"])\n",
    "df1.insert(2, \"address\", address)\n",
    "category = df1[\"type\"] + \" \" + df1[\"class\"].astype(str)\n",
    "df1 = df1.drop(columns=[\"type\", \"class\"])\n",
    "df1.insert(4, \"category\", category)\n",
    "df1.to_csv(folder / \"1_fodors.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"fodors_zagat_raw_data\" / \"tableB.csv\")\n",
    "df2 = df2.map(lambda x: x.strip(\"'\").replace(\"\\\\'\", \"'\") if isinstance(x, str) else x)\n",
    "df2 = df2.drop(columns=[\"name\"])\n",
    "df2.to_csv(folder / \"2_zagats.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"fodors_zagat_raw_data\" / \"matches.csv\")\n",
    "df.rename(columns={\"fodors_id\": \"id1\", \"zagats_id\": \"id2\"}, inplace=True)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs\n",
    "folder = Path(\"../data/blocking/songs/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"msd.csv\")\n",
    "df1.to_csv(folder / \"1_msd.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"matches_msd_msd.csv\")\n",
    "df = df[df[\"id1\"] != df[\"id2\"]]\n",
    "df = df[df[\"id1\"] < df[\"id2\"]]\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "# df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type='broadcast')\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# citeseer-dblp\n",
    "folder = Path(\"../data/blocking/citeseer-dblp/\")\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"citeseer.csv\", low_memory=False)\n",
    "df1.to_csv(folder / \"1_citeseer.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df2 = pd.read_csv(folder / \"raw\" / \"dblp.csv\", low_memory=False)\n",
    "df2.to_csv(folder / \"2_dblp.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"matches_citeseer_dblp.csv\")\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from jnius import autoclass\n",
    "\n",
    "EntitySerializationReader = autoclass(\n",
    "    \"org.scify.jedai.datareader.entityreader.EntitySerializationReader\"\n",
    ")\n",
    "GtSerializationReader = autoclass(\n",
    "    \"org.scify.jedai.datareader.groundtruthreader.GtSerializationReader\"\n",
    ")\n",
    "\n",
    "\n",
    "def profile2df(path):\n",
    "    entitySerializationReader = EntitySerializationReader(path)\n",
    "    entityProfiles = entitySerializationReader.getEntityProfiles()\n",
    "    profilesIterator = entityProfiles.iterator()\n",
    "    profiles = []\n",
    "    while profilesIterator.hasNext():\n",
    "        profile = profilesIterator.next()\n",
    "        pf = {\"id\": len(profiles)}\n",
    "        attributesIterator = profile.getAttributes().iterator()\n",
    "        while attributesIterator.hasNext():\n",
    "            attribute = attributesIterator.next()\n",
    "            pf[attribute.getName()] = attribute.getValue()\n",
    "        profiles.append(pf)\n",
    "\n",
    "    return pd.DataFrame(profiles)\n",
    "\n",
    "\n",
    "def duplicates2df(path):\n",
    "    gtSerializationReader = GtSerializationReader(path)\n",
    "    duplicatePairs = gtSerializationReader.getDuplicatePairs(None)\n",
    "    duplicatePairsIterator = duplicatePairs.iterator()\n",
    "    duplicates = []\n",
    "    while duplicatePairsIterator.hasNext():\n",
    "        idDuplicates = duplicatePairsIterator.next()\n",
    "        duplicate = {\n",
    "            \"id1\": idDuplicates.getEntityId1(),\n",
    "            \"id2\": idDuplicates.getEntityId2(),\n",
    "        }\n",
    "        duplicates.append(duplicate)\n",
    "\n",
    "    return pd.DataFrame(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb-dbpedia\n",
    "folder = Path(\"../data/blocking/imdb-dbpedia\")\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/imdbProfiles\"\n",
    "df1 = profile2df(file)\n",
    "df1 = df1[[\"id\", \"title\", \"starring\", \"writer\", \"editor\"]]\n",
    "df1.to_csv(folder / \"1_imdb.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/dbpediaProfiles\"\n",
    "df2 = profile2df(file)\n",
    "df2 = df2[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"director name\",\n",
    "        \"actor name\",\n",
    "        \"year\",\n",
    "        \"imdb_ksearch_id\",\n",
    "        \"genre\",\n",
    "        \"url\",\n",
    "    ]\n",
    "]\n",
    "df2.to_csv(folder / \"2_dbpedia.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/moviesIdDuplicates\"\n",
    "df = duplicates2df(file)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies\n",
    "def url_suffix(s):\n",
    "    return s.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "folder = Path(\"../data/blocking/movies\")\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/imdbProfilesNEW\"\n",
    "df1 = profile2df(file)\n",
    "print(len(df1.columns))\n",
    "len1 = len(df1)\n",
    "\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/tmdbProfiles\"\n",
    "df2 = profile2df(file)\n",
    "len2 = len(df2)\n",
    "print(len(df2.columns))\n",
    "\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/tvdbProfiles\"\n",
    "df3 = profile2df(file)\n",
    "len3 = len(df3)\n",
    "print(len(df3.columns))\n",
    "\n",
    "df2[\"id\"] += len1\n",
    "df3[\"id\"] += len1 + len2\n",
    "\n",
    "df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "df = df.rename(url_suffix, axis=\"columns\")\n",
    "df.to_csv(folder / \"1_movies.csv\", index=False)\n",
    "print(df.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "file1 = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/imdbTmdbIdDuplicates\"\n",
    "dp1 = duplicates2df(file1)\n",
    "dp1[\"id2\"] += len1\n",
    "\n",
    "file2 = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/imdbTvdbIdDuplicates\"\n",
    "dp2 = duplicates2df(file2)\n",
    "dp2[\"id2\"] += len1 + len2\n",
    "\n",
    "file3 = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/tmdbTvdbIdDuplicates\"\n",
    "dp3 = duplicates2df(file3)\n",
    "dp3[\"id1\"] += len1\n",
    "dp3[\"id2\"] += len1 + len2\n",
    "\n",
    "dp = pd.concat([dp1, dp2, dp3], axis=0, ignore_index=True)\n",
    "assert dp[~(dp[\"id1\"].isin(df[\"id\"]) & dp[\"id2\"].isin(df[\"id\"]))].empty\n",
    "# dp = dp.apply(lambda r: swap_id(r, df), axis=1, result_type='broadcast')\n",
    "assert dp.equals(dp.apply(lambda r: swap_id(r, df), axis=1, result_type=\"broadcast\"))\n",
    "dp.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# census\n",
    "folder = Path(\"../data/blocking/census/\")\n",
    "file = \"../src/vendor/JedAIToolkit/data/dirtyErDatasets/censusProfiles\"\n",
    "df1 = profile2df(file)\n",
    "attrs = list(df1.columns)\n",
    "attrs[1:] = sorted(attrs[1:])\n",
    "df1 = df1[attrs]\n",
    "df1.to_csv(folder / \"1_census.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "file = \"../src/vendor/JedAIToolkit/data/dirtyErDatasets/censusIdDuplicates\"\n",
    "df = duplicates2df(file)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "# df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type='broadcast')\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook\n",
    "folder = Path(\"../data/blocking/notebook/\")\n",
    "\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"X.csv\")\n",
    "df1 = df1.rename(columns={\"instance_id\": \"id\"})\n",
    "title = df1[\"title\"]\n",
    "df1 = df1.drop(columns=[\"title\"])\n",
    "df1.insert(1, \"title\", title)\n",
    "df1.to_csv(folder / \"1_notebook.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"Y.csv\")\n",
    "df = df[df[\"label\"] == 1][[\"left_instance_id\", \"right_instance_id\"]]\n",
    "df = df.rename(columns={\"left_instance_id\": \"id1\", \"right_instance_id\": \"id2\"})\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\")\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook2\n",
    "folder = Path(\"../data/blocking/notebook2/\")\n",
    "\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"X.csv\")\n",
    "df1.to_csv(folder / \"1_notebook.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"Y.csv\")\n",
    "df = df.rename(columns={\"lid\": \"id1\", \"rid\": \"id2\"})\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\")\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altosight\n",
    "folder = Path(\"../data/blocking/altosight/\")\n",
    "\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"X.csv\")\n",
    "df1 = df1.rename(columns={\"instance_id\": \"id\"})\n",
    "df1 = df1[[\"id\", \"name\", \"price\", \"brand\", \"size\"]]\n",
    "df1.to_csv(folder / \"1_altosight.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"Y.csv\")\n",
    "df = df[df[\"label\"] == 1][[\"left_instance_id\", \"right_instance_id\"]]\n",
    "df = df.rename(columns={\"left_instance_id\": \"id1\", \"right_instance_id\": \"id2\"})\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "# df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type='broadcast')\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altosight2\n",
    "folder = Path(\"../data/blocking/altosight2/\")\n",
    "\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"X.csv\")\n",
    "df1.to_csv(folder / \"1_altosight.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"Y.csv\")\n",
    "df = df.rename(columns={\"lid\": \"id1\", \"rid\": \"id2\"})\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\")\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_full\n",
    "folder = Path(\"../data/blocking/notebook_full/\")\n",
    "\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"X.csv\")\n",
    "df1.to_csv(folder / \"1_notebook.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"Y.csv\")\n",
    "df = df.rename(columns={\"lid\": \"id1\", \"rid\": \"id2\"})\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\")\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altosight_full\n",
    "folder = Path(\"../data/blocking/altosight_full/\")\n",
    "\n",
    "df1 = pd.read_csv(folder / \"raw\" / \"X.csv\")\n",
    "df1.to_csv(folder / \"1_altosight.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "\n",
    "df = pd.read_csv(folder / \"raw\" / \"Y.csv\")\n",
    "df = df.rename(columns={\"lid\": \"id1\", \"rid\": \"id2\"})\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "df = df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\")\n",
    "assert df.equals(df.apply(lambda r: swap_id(r, df1), axis=1, result_type=\"broadcast\"))\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdds\n",
    "folder = Path(\"../data/blocking/cds/\")\n",
    "file = \"../src/vendor/JedAIToolkit/data/dirtyErDatasets/cddbProfiles\"\n",
    "df1 = profile2df(file)\n",
    "df1.to_csv(folder / \"1_cds.csv\", index=False)\n",
    "\n",
    "file = \"../src/vendor/JedAIToolkit/data/dirtyErDatasets/cddbIdDuplicates\"\n",
    "df = duplicates2df(file)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df1[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restaurants\n",
    "folder = Path(\"../data/blocking/restaurants\")\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/restaurant1Profiles\"\n",
    "df1 = profile2df(file)\n",
    "df1.rename(columns=lambda c: c.split(\"#\")[-1], inplace=True)\n",
    "df1.to_csv(folder / \"1_resturant.csv\", index=False)\n",
    "print(df1.map(lambda x: len(str(x))).describe())\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/restaurant2Profiles\"\n",
    "df2 = profile2df(file)\n",
    "df2.rename(columns=lambda c: c.split(\"#\")[-1], inplace=True)\n",
    "df2.to_csv(folder / \"2_resturant.csv\", index=False)\n",
    "print(df2.map(lambda x: len(str(x))).describe())\n",
    "file = \"../src/vendor/JedAIToolkit/data/cleanCleanErDatasets/restaurantsIdDuplicates\"\n",
    "df = duplicates2df(file)\n",
    "assert df[~(df[\"id1\"].isin(df1[\"id\"]) & df[\"id2\"].isin(df2[\"id\"]))].empty\n",
    "df.to_csv(folder / \"matches.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
